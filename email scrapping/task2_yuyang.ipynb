{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "headers = {\"Authorization\": \"Bearer token\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"I am yuyang, my name is : \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I am yuyang, my name is : 19D 12  (1 9 digits Binary)\\n\\nI am trying to calculate the probability that if I accidentally hit a random 2-digit number, it will be a multiple of 10.\\n\\nLet me see how this works...\\n\\nIf it is a multiple of 10, it means that if the last 2 bits of the binary number, say, i (i=0,1), are set to 1, it will be a multiple of 10.\\n\\nSo, i(0'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Kurt and Andrea,\n",
      "\n",
      "We have an upcoming paper about to be published that will be something the media might be interested in. I've worked with you both before, so thought I'd reach out re next steps, but please let me know if there is someone else I should be contacting.\n",
      "\n",
      "The paper will be published in\n",
      "\n",
      "The work is on predicting the survival of cancer patients using a branch of artificial intelligence called natural language processing, the same stuff that everyone now knows about thanks to ChatGPT. We use the AI (language models) to predict how long a cancer patient will survive based on the first consultation report their medical oncologists writes after their initial appointment. This is a particularly exciting technique because all cancer patients would have this document, so this prediction is accessible. Our work was able to predict survival quite accurately, similar to prior work that has had to use fancy stuff like tumour genetic markers to make these predictions.\n",
      "\n",
      "I thought the media might be interested, but totally no problem if you think not. If you do, please let me know next steps, what you'd suggest, and the like. I'm very new to anything having to do with media relations :)\n",
      "\n",
      "Best,\n",
      "\n",
      "John-Jose\n",
      "\n",
      "--\n",
      "\n",
      "John-Jose Nu\n",
      "\n",
      "ñez, MD, MSc, FRCPC\n",
      "\n",
      "Clinical Research Fellow\n",
      "\n",
      "Mood Disorders Centre and BC Cancer\n",
      "\n",
      "University of British Columbia\n"
     ]
    }
   ],
   "source": [
    "# get donload_loader\n",
    "from llama_index.core import download_loader\n",
    "# Create a download loader\n",
    "from llama_index.readers.file import UnstructuredReader\n",
    "\n",
    "# Initialize the UnstructuredReader\n",
    "loader = UnstructuredReader()\n",
    "msg_documents = loader.load_data(\"data/safe_on_cloud/jj_email_1.eml\")\n",
    "msg_content = msg_documents[0].text\n",
    "print(msg_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"sender\": \"John-Jose Nuñez\",\n",
      "  \"receiver\": \"Kurt and Andrea\",\n",
      "  \"subject\": \"Next steps for media relations\",\n",
      "  \"requested task\": \"Produce a JSON with the following data: sender, receiver, subject and a field called 'requested task'\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "prompt = msg_content+\"please according this email content, You extract data and returns it in JSON format, for your JSON, you can just produce a JSON that has the sender, receiver, subjectand then a field called 'requested task'.Please remeber: do not include message body\"\n",
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='gemma:2b',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
